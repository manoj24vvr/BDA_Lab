
def non_parametric_func(){
     | println("Hi This is a Non-parametric function")
     | }
non_parametric_func()
Hi This is a Non-parametric function

def parametric_func(num1:Int,num2:Int){
     | println("Sum of given no's: ",num1+num2)
     | }
parametric_func(5,34)
(Sum of given no's: ,39)

def recursive(num:Int):Int={
     | if(num==1){
     | return 1
     | }
     | num*recursive(num-1)
     | }
recursive(6)
res2: Int = 720

def default_param(f1:Double,num:Int, n:Int=10):Double={
     | return f1*n+num
     | }
default_param(12.5,2)
res7: Double = 127.0

def outer(){
     | println("Executing Outer func")
     | def inner(){
     | println("Executing Inner func")
     | }
     | inner()
     | }
outer()


val sum=(a:Int,b:Int)=>a+b
sum: (Int, Int) => Int = $Lambda$1963/1476236281@3b47178c

scala> sum(6,99)
res0: Int = 105

def sum(a:Int)(b:Int)={
     | a+b
     | }
sum: (a: Int)(b: Int)Int

scala> sum(6)(56)
res1: Int = 62


Q7 A Demonstrate the built-in functions using Scala by creating an array. Perform following functions:

val arr= Array(2,4,6,8,10)
arr: Array[Int] = Array(2, 4, 6, 8, 10)

1) Non-Parameterized functions

def non_param(){
     | for(i <- arr){
     | println(i)
     | }
     | }
non_param()
2
4
6
8
10


2) Parameterized functions

def param(arr1:Array[Int]):Array[Int]={
     | for (i <- arr) yield i
     | }
param(arr)
res9: Array[Int] = Array(2, 4, 6, 8, 10)

3) Recursion



4) Function parameter with Default value


5) Anonymous functions



6) Function with Map method



7) Currying functions

def currying(arr1:Array[Int]):Int={
     | var sum=0
     | for (i <- arr1){
     | sum += i
     | }
     | return sum
     | }
currying(arr)
res3: Int = 30

8) Nested functions


B 1) Use spark Scala to demonstrate the add & multiplication of two numbers using Currying

def add_multiply(a:Int)(b:Int){
     | println("Sum: ",a+b)
     | println("Product: ",a*b)
     | }
add_multiply: (a: Int)(b: Int)Unit

scala> add_multiply(6)(8)
(Sum: ,14)
(Product: ,48)


2) Execute Scala Program of Partial function using Case statement by creating array.



3) Create Scala program to demonstrate the function overloading by changing the number of parameters



4) Scala program of Multiple Nested Function



val input = Seq(Some(3), None, Some(-1), None, Some(4), Some(5)) 
Method 1 - collect

input.collect {
  case Some(value) => value * 2
} 
-----------------------
Week6 Q3


MATH Operation

val df = Seq(   (1, 2),   (3, 4),   (5, 6),   (7, 8) ).toDF("col1", "col2")
val sumDF = df.withColumn("sum", $"col1" + $"col2")

AGG operation

val df = Seq(("John", 25),("Jane",30),("Bob", 28),("Alicew",28),("Tom", 27)).toDF("name", "age")

avg() - val avgAgeDF = df.agg(avg("age"))
approx_count_distinct() - val approxDistinctDF = df.agg(approx_count_distinct("age"))
collect_list() - val collectedListdf = df.groupBy("name").agg(collect_list("age"))
collect_set() - val collectedSetDF = df.groupBy("name").agg(collect_set("age"))
count() - val totalRowsDF = df.count()
countDistinct() - val distinctCountDF = df.agg(countDistinct("age"))

------------------------------

Scala commands

val text=sc.textFile("/user/in")
val count = text.flatMap(line=>line.split(" "))
count.collect;
val mapf=count.map(word=>(word,1))
val reducerf = mapf.reduceByKey(_+_)
reducerf.collect;
val countf = mapf.countByValue()

q1. find sum of employees department wise
q2. find the tuples based on class sensitivity

Send dataset to hdfs dfs -copyfromlocal /batch_4/200968174/employees.csv
get data from hdfs
val df = spark.read.option("header","true").csv("/batch_4/200968174/employees.csv") 

change datatype 
df.select(col("salary").cast("int").as("salary")) //skip this command, not necessary might not work also
							do the neeche ka thing instead

answer to q1
df.groupBy("DEPARTMENT_ID").agg(sum("SALARY")).show(false)

answer to q2
df2.groupBy("DEPARTMENT_ID", "GENDER").agg(count("GENDER")).show

-------------------------------------

